## 7 Theory of Hard-Margin Support Vector Machines

### Dual Optimization Problem

Consider the optimization problem below:
$$
\min f(x)
$$

$$
s.t.\quad g_{k}(x)\leq 0
$$

Lagrange multiplier:
$$
\mathcal{L}(x,u)=f(x)+\sum_{k=1}^{n}\lambda_{k}g_{k}(x),\quad \lambda_{k}\geq0,g_{k}(x)\leq 0
$$
Obviously:
$$
\max_{\lambda}\mathcal{L}(x,\lambda)=f(x)
$$

$$
\min_{x}f(x)=\min_{x}\max_{\lambda}\mathcal{L}(x,\lambda)
$$

Then:
$$
\begin{equation}
\begin{split}
\max_{\lambda}\min_{x}\mathcal{L}(x,\lambda)&=\max_{\lambda}\min_{x}f(x)+\max_{\lambda}\min_{x}\lambda g(x) \\
&=\min_{x}f(x)+\max_{\lambda}\min_{x}\lambda g(x)\\
\end{split}
\end{equation}
$$
We can prove that:
$$
\begin{equation}
\min_{x}\lambda g(x)=
\left\{
             \begin{array}{lr}
            0, & \lambda=0 \quad or\quad g(x)=0 \\
            -\infty, & \lambda>0 \quad and\quad g(x)<0
             \end{array}
\right.
\end{equation}
$$
Therefore, when $\lambda=0 $ or $ g(x)=0$:
$$
\min_{x}f(x)=\min_{x}\max_{\lambda}\mathcal{L}(x,\lambda)=\max_{\lambda}\min_{x}\mathcal{L}(x,\lambda)
$$

### Karush-Kuhn-Tucker (KKT) conditions

Consider the standard constraint optimization problem:
$$
\min f(\mathbf{x})
$$

$$
\begin{equation}
\begin{split}
s.t.\quad&g_{j}(\mathbf{x})=0 \\
&h_{k}(\mathbf{x})\leq0\\
\end{split}
\end{equation}
$$

Define Lagrange multiplier:
$$
\mathcal{L}(\mathbf{x},\lambda,\mu)=f(x)+\sum_{j=1}^{m}\lambda_{j}g_{j}(\mathbf{x})+\sum_{k=1}^{p}\mu_{k}h_{k}(\mathbf{x})
$$
KKT conditions:
$$
\begin{equation}
\left\{
             \begin{array}{lr}
            \nabla_{\mathbf{x}}\mathcal{L}=0 \\
            g_{j}(\mathbf{x})=0 \\
            h_{k}(\mathbf{x})\leq 0\\
            \mu_{k}\geq 0\\
            \mu_{k}h_{k}(\mathbf{x})=0
             \end{array}
\right.
\end{equation}
$$

### Hard-Margin Support Vector Machines

Equivalent optimization problem rephrased by dual optimization problem:
$$
\max_{\lambda_{i}\geq 0}\min_{w,\alpha}\|w\|^2-\sum_{i=1}^n\lambda_{i}[y_{i}(X_{i}\cdot w+\alpha)-1]
$$
To solve the inner optimization problem, define Lagrange multiplier:
$$
\mathcal{L}(w,\alpha,\lambda)=w^Tw-\sum_{i=1}^n\lambda_{i}[y_{i}(w^TX_{i}+\alpha)-1]
$$

$$
\frac{\partial\mathcal{L}}{\partial w}=2w-\sum_{i=1}^n\lambda_{i}y_{i}X_{i}=0
$$

$$
\frac{\partial\mathcal{L}}{\partial \alpha}=-\sum_{i=1}^n\lambda_{i}y_{i}=0
$$

Finally, we get:
$$
\max_{\lambda_{i}\geq 0}\sum_{i=1}^n\lambda_{i}-\frac{1}{4}\sum_{i=1}^n\sum_{=1}^n\lambda_{i}\lambda_{j}y_{i}y_{j}X_{i}X_{j},\quad s.t.\sum_{i=1}^n\lambda_{i}y_{i}=0
$$
